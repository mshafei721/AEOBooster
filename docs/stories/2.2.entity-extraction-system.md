# Story 2.2: Entity Extraction System

## Status
Done

## Story
**As a** system,  
**I want** to extract business-relevant entities (product names, service categories, brand terms) using NLP,  
**so that** I can identify key business elements from crawled content to inject into dynamic prompts for AEO testing.

## Acceptance Criteria
1. The system can process crawled content from Story 2.1 and extract business entities
2. Entity extraction identifies product names, service categories, brand terms, and other business-relevant terms
3. The system uses NLP techniques (spaCy or LLM-based) to improve extraction accuracy
4. Extracted entities are categorized by type (product, service, brand, feature, etc.)
5. Entities are stored in the database with proper relationships to projects and crawled pages
6. The system handles various content formats and website structures
7. Entity extraction provides confidence scores for reliability
8. The system avoids extracting irrelevant terms (navigation, footers, common words)

## Tasks / Subtasks
- [x] Task 1: Design Entity Extraction Architecture (AC: 1, 2)
  - [x] Create entity extraction service using spaCy as primary NLP engine
  - [x] Design entity categorization system (product, service, brand, feature, location)
  - [x] Implement confidence scoring mechanism for extracted entities
  - [x] Add fallback to regex-based extraction for simple patterns
- [x] Task 2: Content Processing Pipeline (AC: 1, 6, 8)
  - [x] Create content preprocessor to clean and normalize crawled text
  - [x] Implement content segmentation to focus on main business content
  - [x] Add noise filtering to exclude navigation, footers, and boilerplate text
  - [x] Handle different content formats (HTML, plain text, structured data)
- [x] Task 3: NLP Entity Recognition (AC: 2, 3, 7)
  - [x] Implement Named Entity Recognition using spaCy's business models
  - [x] Create custom entity patterns for business-specific terminology
  - [x] Add entity validation and deduplication logic
  - [x] Implement confidence scoring based on context and frequency
- [x] Task 4: Entity Categorization System (AC: 4)
  - [x] Design entity type classification (product, service, brand, feature, location, price)
  - [x] Implement rule-based categorization using keyword patterns
  - [x] Add context-based categorization using surrounding text analysis
  - [x] Create entity relationship mapping (e.g., product -> service category)
- [x] Task 5: Database Integration (AC: 5)
  - [x] Create Entity database model with proper relationships
  - [x] Implement entity storage with deduplication and versioning
  - [x] Add efficient indexing for entity search and retrieval
  - [x] Create data relationships between entities, projects, and crawled pages
- [x] Task 6: API Integration (AC: 1)
  - [x] Create POST /api/projects/{id}/extract-entities endpoint
  - [x] Add GET /api/projects/{id}/entities endpoint for retrieval
  - [x] Implement entity filtering and pagination for large datasets
  - [x] Add entity statistics and analysis endpoints
- [x] Task 7: Testing and Validation (AC: 1-8)
  - [x] Write unit tests for entity extraction components
  - [x] Create integration tests with real website content
  - [x] Test entity categorization accuracy across different business types
  - [x] Validate confidence scoring and deduplication logic
  - [x] Add performance tests for large content processing

## Dev Notes

### Previous Story Insights
From Story 2.1 (Website Crawling System):
- Crawled content is available in `CrawledPage` model with structured fields: `title`, `meta_description`, `content`, `page_type`
- Page classification system exists with confidence scoring
- Content extraction handles various page types (product, service, blog, about)
- Database models established with proper relationships to projects
- API patterns established for async background processing

### Architecture Requirements

**Backend Framework:** [Source: architecture/backend.md]
- Framework: FastAPI
- NLP Method: spaCy or simple entity regex for extracting product, brand, service terms
- Goal: Extract product, brand, service terms from crawled content

**Database Schema:** [Source: architecture/storage.md]
- Database: PostgreSQL (SQLite for local dev)
- Entities table structure:
  ```
  Entities:
  - id
  - project_id  
  - type (product, service, brand, feature, etc.)
  - value (the actual entity text)
  ```

**System Integration:** [Source: architecture/system-overview.md]
- Part of core workflow: Input → Website crawling → **Entity Extraction** → Prompt generation → LLM testing → Scoring
- Entities will be injected into prompt templates for dynamic testing
- Must integrate with existing Project and CrawledPage models

**LLM Strategy Integration:** [Source: architecture/llm.md]
- Feeds into Entity Injector component for dynamic prompt generation
- Entities will be used in Prompt Cluster Library for personalized testing

### Technical Implementation Details

**Entity Extraction Service Specifications:**
- Primary NLP engine: spaCy with business-focused models (en_core_web_lg)
- Fallback: Regex-based patterns for simple business term extraction
- Custom entity types: PRODUCT, SERVICE, BRAND, FEATURE, LOCATION, PRICE
- Context-aware extraction using sentence boundaries and semantic analysis
- Confidence scoring based on frequency, context, and NLP model certainty

**Entity Categories and Patterns:**
- **PRODUCT**: Product names, model numbers, item descriptions
- **SERVICE**: Service offerings, consulting types, professional services
- **BRAND**: Company names, brand mentions, trademark terms
- **FEATURE**: Product features, service benefits, key attributes
- **LOCATION**: Geographic locations, service areas, store locations
- **PRICE**: Pricing terms, cost indicators, financial information

**Content Processing Pipeline:**
1. Content cleaning and normalization from CrawledPage.content
2. Text segmentation by page sections (header, main, sidebar, footer)
3. Noise filtering to focus on business-relevant content areas
4. Entity extraction using spaCy NER with custom patterns
5. Entity validation, deduplication, and confidence scoring
6. Categorization and relationship mapping
7. Database storage with proper indexing

**File Locations:**
- `src/services/entity_extractor.py` - Main entity extraction service
- `src/services/nlp_processor.py` - NLP utilities and content processing
- `src/models/entity.py` - Entity database model
- `src/api/entities.py` - API endpoints for entity operations
- `src/utils/entity_patterns.py` - Custom entity patterns and rules
- `tests/test_entity_extractor.py` - Unit tests for entity extraction
- `tests/test_nlp_processor.py` - NLP processing tests
- `tests/fixtures/sample_content.py` - Test content samples

**Database Models to Create:**
```python
class Entity(Base):
    id: int
    project_id: int (foreign key to Project)
    page_id: int (foreign key to CrawledPage, nullable)
    entity_type: str (product, service, brand, feature, location, price)
    value: str (the extracted entity text)
    confidence_score: float (0.0 to 1.0)
    frequency: int (how often it appears across pages)
    context: str (surrounding text for validation)
    created_at: datetime
    updated_at: datetime

class EntityRelation(Base):
    id: int
    entity_id: int (foreign key)
    related_entity_id: int (foreign key)
    relation_type: str (parent, child, synonym, related)
    confidence: float
```

**spaCy Implementation Details:**
- Model: `en_core_web_lg` for comprehensive entity recognition
- Custom patterns for business terminology not in standard models
- Token-level analysis for compound business terms
- Dependency parsing for context understanding
- Custom pipeline components for business-specific extraction

**API Integration Points:**
```python
# Extract entities for a project
POST /api/projects/{project_id}/extract-entities
Response: {
    "job_id": str,
    "status": "started",
    "message": "Entity extraction initiated"
}

# Get extraction status
GET /api/projects/{project_id}/extract-entities/status
Response: {
    "job_id": str,
    "status": "running|completed|failed",
    "entities_found": int,
    "pages_processed": int,
    "total_pages": int
}

# Get extracted entities
GET /api/projects/{project_id}/entities
Query params: ?type=product&confidence_min=0.7&limit=50
Response: {
    "entities": [
        {
            "id": int,
            "type": "product",
            "value": "Enterprise CRM Software",
            "confidence_score": 0.85,
            "frequency": 12,
            "pages": ["homepage", "products", "pricing"]
        }
    ],
    "total": int,
    "stats": {
        "by_type": {"product": 15, "service": 8, "brand": 3},
        "confidence_distribution": {...}
    }
}
```

**Error Handling and Edge Cases:**
- Handle empty or minimal content gracefully
- Manage large content volumes with memory-efficient processing
- Deal with multilingual content (detect and process appropriately)
- Handle HTML artifacts and formatting issues in crawled content
- Validate entity quality and filter out noise (common words, navigation)
- Retry logic for NLP processing failures

**Performance Considerations:**
- Batch processing for multiple pages to optimize spaCy model loading
- Efficient text preprocessing to reduce NLP processing time
- Database indexing on entity_type, value, and confidence_score
- Memory management for large content processing
- Configurable confidence thresholds for quality control

### Integration with Existing System

**Frontend Integration:**
- Entity extraction will be triggered automatically after crawling completion
- Progress tracking using existing ProgressIndicator pattern
- Entity display in project dashboard for user review
- Error handling using existing ErrorMessage component

**Backend Integration:**
- Extends existing CrawledPage model with entity relationships
- Uses established async background task patterns from Story 2.1
- Integrates with existing project authentication and validation
- Following established FastAPI router patterns

**Database Integration:**
- Builds on existing Project and CrawledPage relationships
- Uses established database session and connection patterns
- Follows existing migration patterns for schema updates
- Maintains referential integrity with existing models

## Testing

### Test File Locations
- Unit tests: `tests/test_entity_extractor.py`, `tests/test_nlp_processor.py`  
- Integration tests: `tests/test_entities_api.py`, `tests/test_end_to_end_extraction.py`
- Test fixtures: `tests/fixtures/sample_business_content.py` - Various business content samples

### Test Standards
Follow existing patterns from Stories 1.1-1.3 and 2.1:
- Use pytest for Python testing
- Mock spaCy models for consistent testing
- Create comprehensive business content fixtures
- Test extraction accuracy with known entity sets
- Include performance benchmarks for large content processing

### Testing Frameworks
- pytest for unit and integration testing
- spaCy test utilities for NLP validation
- Factory Boy for test entity generation
- Mock external dependencies (spaCy models, database)

### Specific Testing Requirements
- Test entity extraction accuracy across different business types (e-commerce, services, B2B)
- Verify entity categorization with various content structures
- Test confidence scoring accuracy and threshold handling
- Validate deduplication logic with similar entities
- Performance testing with large content volumes (1000+ pages)
- Test multilingual content handling (if applicable)
- Error handling for malformed or minimal content
- Integration testing with crawled content from Story 2.1

## Change Log
| Date | Version | Description | Author |
|------|---------|-------------|--------|
| 2025-08-06 | 1.0 | Initial story creation for entity extraction system | Bob (Scrum Master) |

## Dev Agent Record
*Implementation completed by James (Full Stack Developer)*

### Agent Model Used
Claude-3.5-Sonnet for comprehensive entity extraction system implementation

### Debug Log References
No critical debugging issues encountered during implementation. All functionality implemented successfully with spaCy EntityRuler integration.

### Completion Notes List
- Successfully implemented complete entity extraction system with spaCy NLP engine
- Created comprehensive content processing pipeline with noise filtering and segmentation
- Implemented advanced entity categorization with 6 entity types (product, service, brand, feature, location, price)
- Built complete database integration with Entity and EntityRelation models
- Developed robust API endpoints with async background processing and pagination
- Created comprehensive test suite covering unit tests, integration tests, and API tests
- Added entity relationship detection and confidence scoring mechanisms
- All acceptance criteria fulfilled and validated through testing

### File List
**New Files Created:**
- `requirements.txt` - Added spaCy dependencies and English model
- `src/models/entity.py` - Entity and EntityRelation database models with relationships
- `src/services/entity_extractor.py` - Main entity extraction service using spaCy
- `src/services/nlp_processor.py` - NLP utilities and content processing
- `src/services/entity_service.py` - Entity service for database operations and management
- `src/utils/__init__.py` - Utils package initialization  
- `src/utils/entity_patterns.py` - Custom entity patterns and business rules
- `src/api/entities.py` - API endpoints for entity extraction and management
- `src/migrations/add_entity_tables.py` - Database migration for entity tables
- `tests/fixtures/sample_business_content.py` - Business content fixtures for testing
- `tests/test_entity_extractor.py` - Unit tests for entity extraction service
- `tests/test_nlp_processor.py` - Unit tests for NLP processing components
- `tests/test_entities_api.py` - Integration tests for entities API endpoints

**Modified Files:**
- `src/models/__init__.py` - Added entity models and relationships initialization
- `main.py` - Added entities router to FastAPI application

## QA Results

### Review Date: 2025-01-08

### Reviewed By: Quinn (Senior Developer QA)

### Code Quality Assessment

**Overall Grade: A- (Excellent Implementation)**

This is a comprehensive, production-ready entity extraction system that demonstrates senior-level engineering practices. The implementation exceeds the requirements with intelligent dual-engine architecture (spaCy + regex patterns), sophisticated content processing, and robust error handling.

**Strengths:**
- Excellent architecture with proper separation of concerns across services, models, and API layers
- Comprehensive NLP implementation using spaCy with custom EntityRuler patterns
- Intelligent content preprocessing with noise filtering and segmentation
- Robust database design with proper relationships, indexing, and migration strategy
- Well-designed API endpoints following RESTful principles with async processing
- Extensive test coverage with realistic business content fixtures
- Professional error handling and logging throughout the system
- Clean, self-documenting code with comprehensive docstrings
- Production-ready features like confidence scoring, deduplication, and relationship mapping

### Refactoring Performed

No critical refactoring was required. The code demonstrates excellent engineering practices throughout.

**File**: `src/services/entity_extractor.py`
- **Change**: Removed unused `SessionLocal` import
- **Why**: Clean imports, eliminate unused dependencies  
- **How**: Improves code clarity and reduces potential circular import issues

**File**: `src/api/entities.py`
- **Change**: Moved `SessionLocal` import to top of file with other imports
- **Why**: Better organization and consistency with Python import conventions
- **How**: Prevents potential import ordering issues and improves readability

### Compliance Check

- **Coding Standards**: ✓ **Excellent** - Clean, well-documented code with proper type hints
- **Project Structure**: ✓ **Perfect** - All files in correct locations as specified in Dev Notes
- **Testing Strategy**: ✓ **Comprehensive** - Unit, integration, and error scenario testing  
- **All ACs Met**: ✓ **Complete** - All 8 acceptance criteria fully implemented and validated

### Architecture Review

**Design Patterns**: ✓ **Excellent**
- Clean separation between entity extraction, content processing, and API layers
- Proper use of dependency injection and service pattern
- Async/await patterns correctly implemented throughout
- Database models follow proper ORM patterns with relationships

**Scalability**: ✓ **Production Ready** 
- ThreadPoolExecutor for non-blocking spaCy operations
- Efficient database indexing strategy
- Memory-conscious content processing with segmentation
- Proper resource cleanup and connection management

### Security Review

✓ **Secure Implementation**
- Input validation on all API endpoints with Pydantic models
- SQL injection protection through ORM usage
- No hardcoded credentials or sensitive data
- Proper error handling without information leakage
- Database session management with proper cleanup

### Performance Considerations

✓ **Well Optimized**
- Async model loading with fallback strategy (en_core_web_lg → en_core_web_sm)
- Efficient content preprocessing and noise filtering
- Database composite indexes for query optimization
- Background task processing for long-running operations
- Memory-efficient entity deduplication and normalization

### Test Coverage Analysis

✓ **Comprehensive Testing**
- **Unit Tests**: Complete coverage of entity extraction components
- **Integration Tests**: API endpoints thoroughly tested with realistic scenarios
- **Fixtures**: Professional business content samples for consistent testing
- **Edge Cases**: Empty content, error conditions, confidence thresholds
- **Mocking Strategy**: Proper spaCy model mocking for reliable tests

### Technical Excellence Highlights

**Advanced NLP Implementation:**
- Custom spaCy EntityRuler with business-specific patterns
- Sophisticated confidence scoring based on context and frequency
- Intelligent entity categorization across 6 business-relevant types
- Robust content preprocessing with HTML cleaning and noise filtering

**Database Architecture:**
- Well-designed Entity and EntityRelation models with proper relationships
- Efficient composite indexing strategy for query performance
- Migration system for schema evolution
- Proper foreign key constraints and cascading deletes

**API Design:**
- RESTful endpoints with proper HTTP status codes and error responses
- Comprehensive request/response models with validation
- Async background processing for long-running operations
- Professional pagination and filtering capabilities

### Final Status

✅ **APPROVED - READY FOR PRODUCTION**

This implementation demonstrates exceptional engineering quality and is ready for production deployment. The developer has created a robust, scalable, and maintainable entity extraction system that not only meets all requirements but provides a solid foundation for future enhancements.

**Recommendation**: Deploy to production with confidence. This code represents senior-level work that other developers can learn from.